{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting cases from Yahoo! Finance with days with no market manipulation\n",
    "The script collects close cases of the same companies so that non-manipulated days were not overlayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matvei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matvei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matvei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matvei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matvei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matvei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matvei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RJF\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "COF\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "SUSS\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- SUSS: No data found for this date range, symbol may be delisted\n",
      "CHTR\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ARSN\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "KVMD\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "GRO\n",
      "[Name                                    GRO\n",
      "Start of MM             2013-03-07 00:00:00\n",
      "End of MM               2013-04-23 00:00:00\n",
      "Start Restriction                       NaN\n",
      "End Restriction         2013-04-23 00:00:00\n",
      "To include                                1\n",
      "Appearance                                1\n",
      "Interval                   48 days 00:00:00\n",
      "SideInterval               31 days 08:00:00\n",
      "Restriction on Start                  False\n",
      "Restriction on End                     True\n",
      "Start                   2012-09-16 16:00:00\n",
      "End                     2013-04-23 00:00:00\n",
      "Name: 0, dtype: object,   Name          Start of MM            End of MM Start Restriction  \\\n",
      "0  GRO  2013-03-07 00:00:00  2013-04-23 00:00:00               NaN   \n",
      "\n",
      "       End Restriction To include Appearance          Interval  \\\n",
      "0  2013-04-23 00:00:00          1          1  48 days 00:00:00   \n",
      "\n",
      "       SideInterval Restriction on Start Restriction on End  \\\n",
      "0  31 days 08:00:00                False               True   \n",
      "\n",
      "                 Start                  End  \n",
      "0  2012-09-16 16:00:00  2013-04-23 00:00:00  ]\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- GRO: No data found for this date range, symbol may be delisted\n",
      "CSSE\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "HELE\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "HELE\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "RARE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import yfinance as yf\n",
    "import string\n",
    "\n",
    "n = 20\n",
    "p = int(n * 7)\n",
    "m = 0.45\n",
    "\n",
    "path = \"\"\n",
    "\n",
    "mmf = pd.read_excel(path + \"Market Manipulation Cases\\\\Market Manipulation Cases.xlsx\")\n",
    "\n",
    "mmf['Interval'] = mmf['End of MM'] - mmf['Start of MM'] + timedelta(days = 1)\n",
    "mmf['SideInterval'] = mmf['Interval']*(1-m)/(2*m) + timedelta(days = 2)\n",
    "\n",
    "mmf['Restriction on Start'] = False\n",
    "mmf['Restriction on End'] = False\n",
    "\n",
    "mmf['Start'] = ''\n",
    "mmf['End'] = ''\n",
    "\n",
    "\n",
    "for i in mmf.index:\n",
    "    row = mmf.loc[i]\n",
    "\n",
    "    if (not pd.isnull(row['Start Restriction'])):\n",
    "        if row['Start Restriction'] > row['Start of MM'] - row['SideInterval'] - timedelta(days = p):\n",
    "            row['Restriction on Start'] = True\n",
    "            row['Start'] = row['Start Restriction']\n",
    "    else:\n",
    "        row['Start'] = row['Start of MM'] - row['SideInterval'] - timedelta(days = p)\n",
    "    \n",
    "    if (not pd.isnull(row['End Restriction'])):\n",
    "        if row['End Restriction'] < row['End of MM'] + row['SideInterval']:\n",
    "            row['Restriction on End'] = True\n",
    "            row['End'] = row['End Restriction']\n",
    "    else:\n",
    "        row['End'] = row['End of MM'] + row['SideInterval']\n",
    "        \n",
    "    mmf.loc[i] = row\n",
    "\n",
    "list_of_names = []\n",
    "list_of_company_cases = []\n",
    "    \n",
    "for i in mmf.index:\n",
    "    name = mmf.loc[i, 'Name']\n",
    "    \n",
    "    if (not name in list_of_names) and mmf.loc[i, 'To include'] == 1:\n",
    "        df = mmf.loc[mmf['Name'] == name]\n",
    "        df = df.sort_values(by = 'Start')\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        list_of_company_cases.append(df)\n",
    "        \n",
    "    list_of_names.append(name)\n",
    "    \n",
    "\n",
    "separated_cases_by_name = []\n",
    "    \n",
    "for company_cases in list_of_company_cases:\n",
    "    \n",
    "    separated_cases = []\n",
    "    \n",
    "    separated_case = [] \n",
    "    \n",
    "    for ind in company_cases.index:\n",
    "        company_case = company_cases.loc[ind]\n",
    "#         print(company_case)\n",
    "        \n",
    "        if ind == 0:\n",
    "            separated_case = [company_case, company_case.to_frame().transpose()]\n",
    "        else:\n",
    "            \n",
    "            to_merge = False\n",
    "            \n",
    "            dif = ''\n",
    "            \n",
    "            if company_case['Restriction on Start'] == True:\n",
    "                if company_case['Start'] <= separated_case[0]['End']:\n",
    "                    print('Here!')\n",
    "                    to_merge = True\n",
    "                    dif = separated_case[0]['End'] - company_case['Start']\n",
    "            else:\n",
    "                if company_case['Start'] + timedelta(days = p) <= separated_case[0]['End']:\n",
    "                    to_merge = True\n",
    "                    dif = separated_case[0]['End'] - company_case['Start'] - timedelta(days = p)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if to_merge:\n",
    "                if separated_case[0]['End'] > company_case['End']:\n",
    "                    separated_case = [separated_case[0], pd.concat([separated_case[1], company_case.to_frame().transpose()])]\n",
    "                else:\n",
    "                    cases_of_separated_case = separated_case[1]\n",
    "                    par_of_separated_case = separated_case[0]\n",
    "\n",
    "                    cases_of_separated_case = pd.concat([cases_of_separated_case, company_case.to_frame().transpose()])\n",
    "\n",
    "                    right_dif = 0\n",
    "                    left_dif = 0\n",
    "\n",
    "                    if pd.isnull(par_of_separated_case['Start Restriction']):\n",
    "                        left_dif = 2 * dif\n",
    "                    else:\n",
    "                        left_dif = par_of_separated_case['Start'] - dif - par_of_separated_case['Start Restriction']\n",
    "\n",
    "                    if pd.isnull(company_case['End Restriction']):\n",
    "                        right_dif = 2 * dif\n",
    "                    else:\n",
    "                        right_dif = company_case['End Restriction'] - company_case['End'] - dif\n",
    "\n",
    "                    par_of_separated_case['End Restriction'] = company_case['End Restriction']\n",
    "\n",
    "                    par_of_separated_case['End'] = company_case['End'] + min(dif/2 + max(timedelta(days = 0), left_dif - dif / 2), right_dif)\n",
    "                    par_of_separated_case['Start'] = par_of_separated_case['Start'] - min(dif/2 + max(timedelta(days = 0), right_dif - dif / 2), left_dif)\n",
    "\n",
    "                    separated_case = [par_of_separated_case, cases_of_separated_case]\n",
    "            else:\n",
    "\n",
    "                separated_cases.append(separated_case)\n",
    "                separated_case = [company_case, company_case.to_frame().transpose()]\n",
    "    \n",
    "    separated_cases.append(separated_case)\n",
    "    separated_cases_by_name.append([separated_case[0]['Name'], separated_cases])\n",
    "    \n",
    "files_by_name = []\n",
    "    \n",
    "for separated_cases in separated_cases_by_name:\n",
    "    name = separated_cases[0]\n",
    "    \n",
    "    files = []\n",
    "    \n",
    "    for i in range(0, len(separated_cases[1])):\n",
    "        separated_case = separated_cases[1][i]\n",
    "        \n",
    "        print(name)\n",
    "        \n",
    "        if name == 'GRO' or name == 'QBA' or name == 'ORG.V':\n",
    "            print(separated_case)\n",
    "            \n",
    "        data = yf.download(name, separated_case[0]['Start'] - timedelta(days = 1), separated_case[0]['End'] + timedelta(days = 1))\n",
    "        \n",
    "        manipulated_list = []\n",
    "        \n",
    "        for x in data.index:\n",
    "            manipulated = False\n",
    "            for case_ind in separated_case[1].index:\n",
    "                case = separated_case[1].loc[case_ind]\n",
    "                if x  >= case['Start of MM'] and x <= case['End of MM']:\n",
    "                    manipulated = True\n",
    "            manipulated_list.append(manipulated)\n",
    "            \n",
    "        data['Manipulated'] = manipulated_list\n",
    "        \n",
    "        indeces_to_delete = data[ data['Volume'] == 0 ].index\n",
    "        data.drop(indeces_to_delete, inplace = True)\n",
    "        \n",
    "        \n",
    "        days_before_manip = len(data.loc[:separated_case[0]['Start of MM']])\n",
    "    \n",
    "        \n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        if days_before_manip > n + separated_case[0]['SideInterval'].days:\n",
    "            data.drop(range(0, days_before_manip - (n + separated_case[0]['SideInterval'].days)), inplace = True)\n",
    "                               \n",
    "        string = path + \"Raw Separate Daily Data\\\\\" + name + \"_\" + str(i) + \".xlsx\"\n",
    "        files.append(string)\n",
    "        data.to_excel(string, index = False)\n",
    "    files_by_name.append([name, files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple ways to represent data for ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization without variance\n",
    "def data_standartization_var1(comp, n):\n",
    "    cols = ['Open','High', 'Low', 'Close', 'Adj Close', 'Volume', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            sets = []\n",
    "            \n",
    "            for l in [0, 1, 2, 3, 4, 5]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            \n",
    "            for prices in sets:\n",
    "                mean = prices.mean()\n",
    "                std = prices.std\n",
    "                \n",
    "                if std == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((i.iloc[n] - mean) / (4 * std)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization with variance\n",
    "def data_standartization_var2(comp, n):\n",
    "    cols = ['Volume', 'Open','High', 'Low', 'Close', 'Adj Close', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            volumes = comp.iloc[d:j + 1, 5].reset_index(drop=True)\n",
    "            sum_of_volumes = volumes.sum(axis=0)\n",
    "            average_volume = sum_of_volumes / (n+1)\n",
    "            \n",
    "            volumes_minus_average = volumes - average_volume\n",
    "            volumes_minus_average_squared = volumes_minus_average ** 2\n",
    "            sum_of_volumes_minus_average_squared = volumes_minus_average_squared.sum(axis=0)\n",
    "            sample_variance = sum_of_volumes_minus_average_squared / n\n",
    "            \n",
    "            if sample_variance == 0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append((comp.iloc[j, 5] - average_volume) / (4 * math.sqrt(sample_variance)))\n",
    "            \n",
    "            sets = []\n",
    "            \n",
    "            for l in [0, 1, 2, 3, 4]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            \n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prices_by_volumes = prices * volumes\n",
    "                sum_of_prices_by_volume = prices_by_volumes.sum(axis=0)\n",
    "                average_price = sum_of_prices_by_volume/sum_of_volumes\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                prices_minus_average_squared_by_volumes = prices_minus_average_squared * volumes\n",
    "                sum_of_prices_minus_average_squared_by_volumes = prices_minus_average_squared_by_volumes.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared_by_volumes / sum_of_volumes\n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((i.iloc[n] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differecnes with variance from adj close\n",
    "def data_standartization_var4(comp, n): \n",
    "    cols = ['Volume','Adj close', 'Open dif', 'High dif', 'Low dif', 'Close dif', 'High Low dif', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            volumes = comp.iloc[d:j + 1, 5].reset_index(drop=True)\n",
    "            sum_of_volumes = volumes.sum(axis=0)\n",
    "            average_volume = sum_of_volumes / (n+1)\n",
    "            \n",
    "            volumes_minus_average = volumes - average_volume\n",
    "            volumes_minus_average_squared = volumes_minus_average ** 2\n",
    "            sum_of_volumes_minus_average_squared = volumes_minus_average_squared.sum(axis=0)\n",
    "            sample_variance = sum_of_volumes_minus_average_squared / n\n",
    "            \n",
    "            if sample_variance == 0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append((comp.iloc[j, 5] - average_volume) / (4 * math.sqrt(sample_variance)))\n",
    "            \n",
    "            sets = []\n",
    "            for l in [4]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            for l in [0, 1, 2, 3]:\n",
    "                sets.append((comp.iloc[d:j + 1, l] - comp.iloc[d:j + 1, 4]).reset_index(drop=True))\n",
    "            sets.append((comp.iloc[d:j + 1, 1] - comp.iloc[d:j + 1, 2]).reset_index(drop=True))\n",
    "            \n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prices_by_volumes = prices * volumes\n",
    "                sum_of_prices_by_volume = prices_by_volumes.sum(axis=0)\n",
    "                average_price = sum_of_prices_by_volume/sum_of_volumes\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                prices_minus_average_squared_by_volumes = prices_minus_average_squared * volumes\n",
    "                sum_of_prices_minus_average_squared_by_volumes = prices_minus_average_squared_by_volumes.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared_by_volumes / sum_of_volumes\n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((i.iloc[n] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differecnes with variance from close\n",
    "def data_standartization_var3(comp, n): \n",
    "    cols = ['Volume','Open', 'Close', 'Open dif', 'High dif', 'Low dif', 'Adj dif', 'High Low dif', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            volumes = comp.iloc[d:j + 1, 5].reset_index(drop=True)\n",
    "            sum_of_volumes = volumes.sum(axis=0)\n",
    "            average_volume = sum_of_volumes / (n+1)\n",
    "            \n",
    "            volumes_minus_average = volumes - average_volume\n",
    "            volumes_minus_average_squared = volumes_minus_average ** 2\n",
    "            sum_of_volumes_minus_average_squared = volumes_minus_average_squared.sum(axis=0)\n",
    "            sample_variance = sum_of_volumes_minus_average_squared / n\n",
    "            \n",
    "            if sample_variance == 0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append((comp.iloc[j, 5] - average_volume) / (4 * math.sqrt(sample_variance)))\n",
    "            \n",
    "            sets = []\n",
    "            for l in [0, 3]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            for l in [0, 1, 2, 4]:\n",
    "                sets.append((comp.iloc[d:j + 1, l] - comp.iloc[d:j + 1, 3]).reset_index(drop=True))\n",
    "            sets.append((comp.iloc[d:j + 1, 1] - comp.iloc[d:j + 1, 2]).reset_index(drop=True))\n",
    "            \n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prices_by_volumes = prices * volumes\n",
    "                sum_of_prices_by_volume = prices_by_volumes.sum(axis=0)\n",
    "                average_price = sum_of_prices_by_volume/sum_of_volumes\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                prices_minus_average_squared_by_volumes = prices_minus_average_squared * volumes\n",
    "                sum_of_prices_minus_average_squared_by_volumes = prices_minus_average_squared_by_volumes.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared_by_volumes / sum_of_volumes\n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((i.iloc[n] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differecnes with variance from adj close with linear regression\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "def data_standartization_var6(comp, n): \n",
    "    cols = ['Volume','Close', 'Open dif', 'High dif', 'Low dif', 'High Low dif', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            volumes = comp.iloc[d:j + 1, 5].reset_index(drop=True)\n",
    "            sum_of_volumes = volumes.sum(axis=0)\n",
    "            average_volume = sum_of_volumes / (n+1)\n",
    "            \n",
    "            volumes_minus_average = volumes - average_volume\n",
    "            volumes_minus_average_squared = volumes_minus_average ** 2\n",
    "            sum_of_volumes_minus_average_squared = volumes_minus_average_squared.sum(axis=0)\n",
    "            sample_variance = sum_of_volumes_minus_average_squared / n\n",
    "            \n",
    "            if sample_variance == 0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append((comp.iloc[j, 5] - average_volume) / (4 * math.sqrt(sample_variance)))\n",
    "            \n",
    "            sets = []\n",
    "            \n",
    "            for l in [3]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            for l in [0, 1, 2]:\n",
    "                reg = linear_model.LinearRegression()\n",
    "                reg.fit(comp.iloc[d:j + 1, 3].values.reshape(-1, 1), comp.iloc[d:j + 1, l].values.reshape(-1, 1), comp.iloc[d:j + 1, 5].array)\n",
    "                reseted = comp.iloc[d:j + 1, l].reset_index(drop=True)\n",
    "                sets.append(reseted - pd.DataFrame(list(reg.predict(comp.iloc[d:j + 1, 3].values.reshape(-1, 1)))).iloc[:, 0])\n",
    "            sets.append((comp.iloc[d:j + 1, 1] - comp.iloc[d:j + 1, 2]).reset_index(drop=True))\n",
    "            \n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prices_by_volumes = prices * volumes\n",
    "                sum_of_prices_by_volume = prices_by_volumes.sum(axis=0)\n",
    "                average_price = sum_of_prices_by_volume/sum_of_volumes\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                prices_minus_average_squared_by_volumes = prices_minus_average_squared * volumes\n",
    "                sum_of_prices_minus_average_squared_by_volumes = prices_minus_average_squared_by_volumes.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared_by_volumes / sum_of_volumes\n",
    "                \n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((i.iloc[n] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differecnes with variance from adj close with linear regression withut weights\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "def data_standartization_var7(comp, n): \n",
    "    cols = ['Volume','Close', 'Open dif', 'High dif', 'Low dif', 'High Low dif', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            volumes = comp.iloc[d:j + 1, 5].reset_index(drop=True)\n",
    "            sum_of_volumes = volumes.sum(axis=0)\n",
    "            average_volume = sum_of_volumes / (n+1)\n",
    "            \n",
    "            volumes_minus_average = volumes - average_volume\n",
    "            volumes_minus_average_squared = volumes_minus_average ** 2\n",
    "            sum_of_volumes_minus_average_squared = volumes_minus_average_squared.sum(axis=0)\n",
    "            sample_variance = sum_of_volumes_minus_average_squared / n\n",
    "            \n",
    "            if sample_variance == 0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append((comp.iloc[j, 5] - average_volume) / (4 * math.sqrt(sample_variance)))\n",
    "            \n",
    "            sets = []\n",
    "            \n",
    "            for l in [3]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            for l in [0, 1, 2]:\n",
    "                reg = linear_model.LinearRegression()\n",
    "                reg.fit(comp.iloc[d:j + 1, 3].values.reshape(-1, 1), comp.iloc[d:j + 1, l].values.reshape(-1, 1))\n",
    "                reseted = comp.iloc[d:j + 1, l].reset_index(drop=True)\n",
    "                sets.append(reseted - pd.DataFrame(list(reg.predict(comp.iloc[d:j + 1, 3].values.reshape(-1, 1)))).iloc[:, 0])\n",
    "            sets.append((comp.iloc[d:j + 1, 1] - comp.iloc[d:j + 1, 2]).reset_index(drop=True))\n",
    "            \n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prices_by_volumes = prices * volumes\n",
    "                sum_of_prices_by_volume = prices_by_volumes.sum(axis=0)\n",
    "                average_price = sum_of_prices_by_volume/sum_of_volumes\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                prices_minus_average_squared_by_volumes = prices_minus_average_squared * volumes\n",
    "                sum_of_prices_minus_average_squared_by_volumes = prices_minus_average_squared_by_volumes.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared_by_volumes / sum_of_volumes\n",
    "                \n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((i.iloc[n] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            if row[1:7] == [0, 0, 0, 0, 0, 0]:\n",
    "                print(j)\n",
    "            \n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of percentage change without variance\n",
    "def data_standartization_var8(comp, n):\n",
    "    cols = ['Volume', 'Open','High', 'Low', 'Close', 'Adj Close', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            sets = []\n",
    "            \n",
    "            for l in [0, 1, 2, 3, 4, 5]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            \n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prev_prices = prices.iloc[0:n].reset_index(drop=True)\n",
    "                new_prices = prices.iloc[1:n+1].reset_index(drop=True)\n",
    "                \n",
    "                prices = (new_prices - prev_prices)*100 / prev_prices\n",
    "                \n",
    "                sum_of_prices = prices.sum(axis=0)\n",
    "                average_price = sum_of_prices/(n+1)\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                sum_of_prices_minus_average_squared = prices_minus_average_squared.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared / n\n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((prices.iloc[n-1] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of percentages with variance\n",
    "def data_standartization_var9(comp, n):\n",
    "    cols = ['Volume', 'Change of Volume', 'Open','High', 'Low', 'Close', 'Adj Close', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            volumes = comp.iloc[d:j + 1, 5].reset_index(drop=True)\n",
    "            \n",
    "            sum_of_volumes = volumes.sum(axis=0)\n",
    "            average_volume = sum_of_volumes / (n+1)\n",
    "            \n",
    "            volumes_minus_average = volumes - average_volume\n",
    "            volumes_minus_average_squared = volumes_minus_average ** 2\n",
    "            sum_of_volumes_minus_average_squared = volumes_minus_average_squared.sum(axis=0)\n",
    "            sample_variance = sum_of_volumes_minus_average_squared / n\n",
    "            \n",
    "            if sample_variance == 0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append((comp.iloc[j, 5] - average_volume) / (4 * math.sqrt(sample_variance)))\n",
    "            \n",
    "            sets = []\n",
    "            \n",
    "            for l in [0, 1, 2, 3, 4, 5]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            \n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prev_prices = prices.iloc[0:n].reset_index(drop=True)\n",
    "                new_prices = prices.iloc[1:n+1].reset_index(drop=True)\n",
    "                \n",
    "                prices = (new_prices - prev_prices) / prev_prices\n",
    "                \n",
    "                prices_by_volumes = prices * volumes\n",
    "                sum_of_prices_by_volume = prices_by_volumes.sum(axis=0)\n",
    "                average_price = sum_of_prices_by_volume/sum_of_volumes\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                prices_minus_average_squared_by_volumes = prices_minus_average_squared * volumes\n",
    "                sum_of_prices_minus_average_squared_by_volumes = prices_minus_average_squared_by_volumes.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared_by_volumes / sum_of_volumes\n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((prices.iloc[n-1] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differecnes with variance from close, adj close is not weighted\n",
    "def data_standartization_var5(comp, n): \n",
    "    cols = ['Adj dif', 'Volume', 'Close', 'Open dif', 'High dif', 'Low dif', 'High Low dif', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            \n",
    "            sets1 = []\n",
    "            sets1.append((comp.iloc[d:j + 1, 4] - comp.iloc[d:j + 1, 3]).reset_index(drop=True))\n",
    "            for l in [5]:\n",
    "                sets1.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            for volumes in sets1:\n",
    "                sum_of_volumes = volumes.sum(axis=0)\n",
    "                average_volume = sum_of_volumes / (n+1)\n",
    "                \n",
    "                volumes_minus_average = volumes - average_volume\n",
    "                volumes_minus_average_squared = volumes_minus_average ** 2\n",
    "                sum_of_volumes_minus_average_squared = volumes_minus_average_squared.sum(axis=0)\n",
    "                sample_variance = sum_of_volumes_minus_average_squared / n\n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((volumes.iloc[n] - average_volume) / (4 * math.sqrt(sample_variance)))\n",
    "            \n",
    "            sets = []\n",
    "            for l in [3]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            for l in [0, 1, 2]:\n",
    "                sets.append((comp.iloc[d:j + 1, l] - comp.iloc[d:j + 1, 3]).reset_index(drop=True))\n",
    "            sets.append((comp.iloc[d:j + 1, 1] - comp.iloc[d:j + 1, 2]).reset_index(drop=True))\n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prices_by_volumes = prices * volumes\n",
    "                sum_of_prices_by_volume = prices_by_volumes.sum(axis=0)\n",
    "                average_price = sum_of_prices_by_volume/sum_of_volumes\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                prices_minus_average_squared_by_volumes = prices_minus_average_squared * volumes\n",
    "                sum_of_prices_minus_average_squared_by_volumes = prices_minus_average_squared_by_volumes.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared_by_volumes / sum_of_volumes\n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((i.iloc[n] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differecnes with variance from close without var\n",
    "def data_standartization_var10(comp, n): \n",
    "    cols = ['Volume','Open', 'Close', 'Open dif', 'High dif', 'Low dif', 'High Low dif', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            volumes = comp.iloc[d:j + 1, 5].reset_index(drop=True)\n",
    "            sum_of_volumes = volumes.sum(axis=0)\n",
    "            average_volume = sum_of_volumes / (n+1)\n",
    "            \n",
    "            volumes_minus_average = volumes - average_volume\n",
    "            volumes_minus_average_squared = volumes_minus_average ** 2\n",
    "            sum_of_volumes_minus_average_squared = volumes_minus_average_squared.sum(axis=0)\n",
    "            sample_variance = sum_of_volumes_minus_average_squared / n\n",
    "            \n",
    "            if sample_variance == 0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append((comp.iloc[j, 5] - average_volume) / (4 * math.sqrt(sample_variance)))\n",
    "            \n",
    "            sets = []\n",
    "            for l in [0, 3]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            for l in [0, 1, 2]:\n",
    "                sets.append((comp.iloc[d:j + 1, l] - comp.iloc[d:j + 1, 3]).reset_index(drop=True))\n",
    "            sets.append((comp.iloc[d:j + 1, 1] - comp.iloc[d:j + 1, 2]).reset_index(drop=True))\n",
    "            \n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prices_by_volumes = prices\n",
    "                sum_of_prices_by_volume = prices_by_volumes.sum(axis=0)\n",
    "                average_price = sum_of_prices_by_volume/(n+1)\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                prices_minus_average_squared_by_volumes = prices_minus_average_squared\n",
    "                sum_of_prices_minus_average_squared_by_volumes = prices_minus_average_squared_by_volumes.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared_by_volumes / n\n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((i.iloc[n] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differecnes with variance from adj close with linear regression\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "def data_standartization_var11(comp, n): \n",
    "    cols = ['Volume','Close', 'Open dif', 'High dif', 'Low dif', 'High Low dif', 'Manipulated']\n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            volumes = comp.iloc[d:j + 1, 5].reset_index(drop=True)\n",
    "            sum_of_volumes = volumes.sum(axis=0)\n",
    "            average_volume = sum_of_volumes / (n+1)\n",
    "            \n",
    "            volumes_minus_average = volumes - average_volume\n",
    "            volumes_minus_average_squared = volumes_minus_average ** 2\n",
    "            sum_of_volumes_minus_average_squared = volumes_minus_average_squared.sum(axis=0)\n",
    "            sample_variance = sum_of_volumes_minus_average_squared / n\n",
    "    \n",
    "            \n",
    "            if sample_variance == 0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append((comp.iloc[j, 5] - average_volume) / (4 * math.sqrt(sample_variance)))\n",
    "            \n",
    "            sets = []\n",
    "            \n",
    "            for l in [3]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            for l in [0, 1, 2]:\n",
    "                reg = linear_model.LinearRegression()\n",
    "                reg.fit(comp.iloc[d:j + 1, 3].values.reshape(-1, 1), comp.iloc[d:j + 1, l].values.reshape(-1, 1), comp.iloc[d:j + 1, 5].array)\n",
    "                reseted = comp.iloc[d:j + 1, l].reset_index(drop=True)\n",
    "                sets.append(reseted - pd.DataFrame(list(reg.predict(comp.iloc[d:j + 1, 3].values.reshape(-1, 1)))).iloc[:, 0])\n",
    "            sets.append((comp.iloc[d:j + 1, 1] - comp.iloc[d:j + 1, 2]).reset_index(drop=True))\n",
    "            \n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prev_prices = prices.iloc[0:n].reset_index(drop=True)\n",
    "                new_prices = prices.iloc[1:n+1].reset_index(drop=True)\n",
    "                \n",
    "                \n",
    "                \n",
    "                prices_by_volumes = prices\n",
    "                sum_of_prices_by_volume = prices_by_volumes.sum(axis=0)\n",
    "                average_price = sum_of_prices_by_volume/(n+1)\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                prices_minus_average_squared_by_volumes = prices_minus_average_squared\n",
    "                sum_of_prices_minus_average_squared_by_volumes = prices_minus_average_squared_by_volumes.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared_by_volumes / n\n",
    "                \n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((i.iloc[n] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_standartization_var12(comp, n): \n",
    "    cols = ['Volume', 'Volume variance',\n",
    "            \n",
    "            'Open with volume', 'Open with volume variance', 'High with volume', 'High with volume variance', 'Low with volume',\n",
    "            'Low with volume variance', 'Close with volume', 'Close with volume variance',\n",
    "            \n",
    "            'Open minus close with volume', 'Open minus close with volume variance', 'High minus close with volume',\n",
    "            'High minus close with volume variance', 'Low minus close with volume', 'Low minus close with volume variance',\n",
    "            \n",
    "            'Volume percentage change', 'Volume percentage change variance',\n",
    "            \n",
    "            'Open percentage change', 'Open percentage change variance', 'High percentage change',\n",
    "            'High percentage change variance', 'Low percentage change', 'Low percentage change variance',\n",
    "            'Close percentage change', 'Close percentage change variance',\n",
    "            \n",
    "#             'Open minus percentage change close', 'Open minus close percentage change variance',\n",
    "#             'High minus close percentage change', 'High minus close percentage change variance', \n",
    "#             'Low minus close percentage change', 'Low minus close percentage change variance',\n",
    "            'Manipulated']\n",
    "    \n",
    "    newcomp = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for j in comp.index:\n",
    "        if j > n - 1:\n",
    "            d = j - n\n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            volumes = comp.iloc[d:j + 1, 5].reset_index(drop=True)\n",
    "            sum_of_volumes = volumes.sum(axis=0)\n",
    "            average_volume = sum_of_volumes / (n+1)\n",
    "            \n",
    "            volumes_minus_average = volumes - average_volume\n",
    "            volumes_minus_average_squared = volumes_minus_average ** 2\n",
    "            sum_of_volumes_minus_average_squared = volumes_minus_average_squared.sum(axis=0)\n",
    "            sample_variance = sum_of_volumes_minus_average_squared / n\n",
    "            \n",
    "            if sample_variance == 0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append((comp.iloc[j, 5] - average_volume) / (4 * math.sqrt(sample_variance)))\n",
    "            row.append(sample_variance)\n",
    "            \n",
    "            sets = []\n",
    "            \n",
    "            for l in [0, 1, 2, 3]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            for l in [0, 1, 2]:\n",
    "                sets.append((comp.iloc[d:j + 1, l] - comp.iloc[d:j + 1, 3]).reset_index(drop=True))\n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                prices_by_volumes = prices * volumes\n",
    "                sum_of_prices_by_volume = prices_by_volumes.sum(axis=0)\n",
    "                average_price = sum_of_prices_by_volume/sum_of_volumes\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                prices_minus_average_squared_by_volumes = prices_minus_average_squared * volumes\n",
    "                sum_of_prices_minus_average_squared_by_volumes = prices_minus_average_squared_by_volumes.sum(axis=0)\n",
    "                sample_variance = sum_of_prices_minus_average_squared_by_volumes / sum_of_volumes\n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((i.iloc[n] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "                row.append(sample_variance)\n",
    "            \n",
    "            sets = []\n",
    "            \n",
    "            for l in [0, 1, 2, 3, 5]:\n",
    "                sets.append(comp.iloc[d:j + 1, l].reset_index(drop=True))\n",
    "            for i in sets:\n",
    "                prices = i\n",
    "                \n",
    "                prev_prices = prices.iloc[0:n].reset_index(drop=True)\n",
    "                new_prices = prices.iloc[1:n+1].reset_index(drop=True)\n",
    "                \n",
    "                prices = (new_prices - prev_prices)*100 / prev_prices\n",
    "                \n",
    "                sum_of_prices = prices.sum(axis=0)\n",
    "                average_price = sum_of_prices/ n\n",
    "                \n",
    "                prices_minus_average = prices - average_price\n",
    "                prices_minus_average_squared = prices_minus_average**2\n",
    "                \n",
    "                sum_of_prices_minus_average_squared = prices_minus_average_squared.sum(axis=0)\n",
    "                \n",
    "                sample_variance = sum_of_prices_minus_average_squared / (n - 1)\n",
    "                \n",
    "                if sample_variance == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    row.append((prices.iloc[n-1] - average_price) / (4 * math.sqrt(sample_variance)))\n",
    "                row.append(sample_variance)\n",
    "\n",
    "            row.append(int(comp.iloc[j, 6]))\n",
    "            \n",
    "            newcomp.loc[len(newcomp)] = row\n",
    "    return newcomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of variants. Combinig all days after deriving new way to present their data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizaing and combining data\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def company_data_standartization(comp, n, standartization_var):\n",
    "    if standartization_var == 1:\n",
    "        return data_standartization_var1(comp, n)\n",
    "    if standartization_var == 2:\n",
    "        return data_standartization_var2(comp, n)\n",
    "    if standartization_var == 3:\n",
    "        return data_standartization_var3(comp, n)\n",
    "    if standartization_var == 4:\n",
    "        return data_standartization_var4(comp, n)\n",
    "    if standartization_var == 5:\n",
    "        return data_standartization_var5(comp, n)\n",
    "    if standartization_var == 6:\n",
    "        return data_standartization_var6(comp, n)\n",
    "    if standartization_var == 7:\n",
    "        return data_standartization_var7(comp, n)\n",
    "    if standartization_var == 8:\n",
    "        return data_standartization_var8(comp, n)\n",
    "    if standartization_var == 9:\n",
    "        return data_standartization_var9(comp, n)\n",
    "    if standartization_var == 10:\n",
    "        return data_standartization_var10(comp, n)\n",
    "    if standartization_var == 11:\n",
    "        return data_standartization_var11(comp, n)\n",
    "    if standartization_var == 12:\n",
    "        return data_standartization_var12(comp, n)\n",
    "    \n",
    "def new_all_data_standartization(n, standartization_var):\n",
    "    \n",
    "    all_stand_cases = []\n",
    "    \n",
    "    for [name, files] in files_by_name:\n",
    "        \n",
    "        stand_cases = []\n",
    "        \n",
    "        for file in files:\n",
    "            case = pd.read_excel(file)\n",
    "            if len(case) > n:\n",
    "                stand_case = company_data_standartization(case.drop('Date', axis=1), n, standartization_var)\n",
    "                \n",
    "                stand_case['Company'] = [name]*len(stand_case)\n",
    "                \n",
    "                dates = pd.DataFrame(case.loc[n:, 'Date'])\n",
    "                dates.reset_index(inplace=True)\n",
    "                stand_case['Date'] = dates['Date']\n",
    "                stand_cases.append(stand_case)\n",
    "        \n",
    "        if len(stand_cases) != 0:\n",
    "            stand_cases_united = pd.concat(stand_cases)\n",
    "            stand_cases_united.sort_values('Manipulated', ascending=False, inplace = True)\n",
    "            stand_cases_united.drop_duplicates(subset = 'Date', keep = \"first\", inplace = True)\n",
    "            all_stand_cases.append(stand_cases_united)\n",
    "        \n",
    "    all_stand_cases_united = pd.concat(all_stand_cases)\n",
    "    all_stand_cases_united.reset_index(drop = True, inplace=True)\n",
    "    \n",
    "    string = path + \"All Normalized Data\\\\Var\" + str(standartization_var) + \"\\\\\" + \"AllDays\" + \".xlsx\"\n",
    "    all_stand_cases_united.to_excel(string, index = False)      \n",
    "    \n",
    "    manip_days = pd.DataFrame(columns = all_stand_cases_united.columns)\n",
    "    not_manip_days = pd.DataFrame(columns = all_stand_cases_united.columns)\n",
    "\n",
    "    for i in all_stand_cases_united.index:\n",
    "        if all_stand_cases_united.loc[i, 'Manipulated'] == 1:\n",
    "            manip_days.loc[len(manip_days)] = all_stand_cases_united.loc[i]\n",
    "        else:\n",
    "            not_manip_days.loc[len(not_manip_days)] = all_stand_cases_united.loc[i]\n",
    "\n",
    "    manip_days.reset_index(drop = True, inplace = True)\n",
    "    not_manip_days.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    manip_days.to_excel(\"All Normalized Data\\\\Var\" + str(standartization_var) + \"\\\\\" + \"Manipulated Days\" + \".xlsx\", index = False)\n",
    "    not_manip_days.to_excel(\"All Normalized Data\\\\Var\" + str(standartization_var) + \"\\\\\" + \"Not Manipulated Days\" + \".xlsx\", index = False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_data_standartization(n, 1)\n",
    "new_all_data_standartization(n, 2)\n",
    "new_all_data_standartization(n, 3)\n",
    "new_all_data_standartization(n, 4)\n",
    "new_all_data_standartization(n, 5)\n",
    "new_all_data_standartization(n, 6)\n",
    "new_all_data_standartization(n, 7)\n",
    "new_all_data_standartization(n, 8)\n",
    "new_all_data_standartization(n, 9)\n",
    "new_all_data_standartization(n, 10)\n",
    "new_all_data_standartization(n, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_data_standartization(n, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting histograms for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_hist(var):\n",
    "    string = path + \"All Normalized Data\\\\Var\" + str(var) + \"\\\\\" + \"AllDays\" + \".xlsx\"\n",
    "    all_stand_cases_united = pd.read_excel(string, index = False)\n",
    "    \n",
    "    for column in all_stand_cases_united.columns.values.tolist()[0:len(all_stand_cases_united.columns) - 3]:\n",
    "        all_stand_cases_united.loc[:, column].hist(bins=80)\n",
    "        plt.xlabel(column)\n",
    "        plt.savefig(\"Hists\\\\Var\" + str(var) + \"\\\\hist_\" + column + \".pdf\", format='pdf')\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hist(1)\n",
    "data_hist(2)\n",
    "data_hist(3)\n",
    "data_hist(4)\n",
    "data_hist(5)\n",
    "data_hist(6)\n",
    "data_hist(7)\n",
    "data_hist(8)\n",
    "data_hist(9)\n",
    "data_hist(10)\n",
    "data_hist(11)\n",
    "data_hist(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting correlation table for the 12th variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "standartization_var = 12\n",
    "path = ''\n",
    "\n",
    "string = path + \"All Normalized Data\\\\Var\" + str(standartization_var) + \"\\\\\" + \"AllDays\" + \".xlsx\"\n",
    "all_stand_cases_united = pd.read_excel(string, index = False)\n",
    "\n",
    "pt = PowerTransformer()\n",
    "pt.fit(all_stand_cases_united.iloc[:, :len(all_stand_cases_united.columns) - 3])\n",
    "new_data = pt.transform(all_stand_cases_united.iloc[:, :len(all_stand_cases_united.columns) - 3])\n",
    "\n",
    "new_data.drop(new_data)\n",
    "\n",
    "cols = all_stand_cases_united.iloc[:, :len(all_stand_cases_united.columns) - 3].columns.values.tolist()\n",
    "\n",
    "corr = pd.DataFrame(new_data, columns = cols).corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr.mask(corr>=0.7, 1)\n",
    "corr = corr.mask(corr<0.7, 0)\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin of scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter_data(standartization_var):\n",
    "    manipulated = pd.read_excel(path + \"All Normalized Data\\\\Var\" + str(standartization_var) + \"\\\\\" + \"Manipulated Days\" + \".xlsx\")\n",
    "    not_manipulated = pd.read_excel(path + \"All Normalized Data\\\\Var\" + str(standartization_var) + \"\\\\\" + \"Not Manipulated Days\" + \".xlsx\")\n",
    "    variables = manipulated.columns.values.tolist()\n",
    "    variables.remove('Manipulated')\n",
    "    variables.remove('Company')\n",
    "    variables.remove('Date')\n",
    "    for combo in list(combinations(variables, 2)):\n",
    "        ax = manipulated.plot.scatter(x = combo[0], y = combo[1], color = 'Red', label='Manipulated', s=0.01)\n",
    "        scatter = not_manipulated.plot.scatter(x = combo[0], y = combo[1], color = 'Green', label='Not Manipulated', s=0.01, ax=ax)\n",
    "        scatter.get_figure().savefig(\"Scatter\\\\Var\" + str(standartization_var) + \"\\\\scatter_\" + combo[0] + \"+\" + combo[1] + \".pdf\", format='pdf')\n",
    "        plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_data(1)\n",
    "scatter_data(2)\n",
    "scatter_data(3)\n",
    "scatter_data(4)\n",
    "scatter_data(5)\n",
    "scatter_data(6)\n",
    "scatter_data(7)\n",
    "scatter_data(8)\n",
    "scatter_data(9)\n",
    "scatter_data(10)\n",
    "scatter_data(11)\n",
    "scatter_data(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that aproximates the results for a any ML method\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def classifier_effectivness(classifier, clas_name, n, standartization_var, remove = [], par = [0], to_print_boxplot = False):\n",
    "    train_frac = 0.5\n",
    "    \n",
    "    s = np.matrix('0.0, 0.0; 0.0, 0.0')\n",
    "    manip_days = pd.read_excel(\"All Normalized Data\\\\Var\" + str(standartization_var) + \"\\\\\" + \"Manipulated Days\" + \".xlsx\")\n",
    "    not_manip_days = pd.read_excel(\"All Normalized Data\\\\Var\" + str(standartization_var) + \"\\\\\" + \"Not Manipulated Days\" + \".xlsx\")\n",
    "    \n",
    "    manip_index = len(manip_days.columns) - 3\n",
    "    company_index = manip_index + 1\n",
    "    \n",
    "    indexes_to_use = list(range(0, manip_index))\n",
    "    \n",
    "    for i in remove:\n",
    "        indexes_to_use.remove(i)\n",
    "\n",
    "    distribution = pd.DataFrame(columns=['Sensitivity', 'Specificity', 'Accuracy'])   \n",
    "    \n",
    "    for i in range(0, n):\n",
    "        manip_days = manip_days.sample(frac=1).reset_index(drop = True)\n",
    "        not_manip_days = not_manip_days.sample(frac=1).reset_index(drop = True)\n",
    "    \n",
    "        x_m = manip_days.iloc[:, indexes_to_use]\n",
    "        x_n = not_manip_days.iloc[:, indexes_to_use].iloc[0:len(x_m)]\n",
    "        \n",
    "        y_m = manip_days.iloc[:, manip_index]\n",
    "        y_n = not_manip_days.iloc[:, manip_index].iloc[0:len(x_m)]\n",
    "\n",
    "        l_m = int(len(x_m)*train_frac)\n",
    "        l_n = int(len(x_n)*train_frac)\n",
    "\n",
    "        x_train = pd.concat([x_m.iloc[:l_m, :], x_n.iloc[:l_n, :]])\n",
    "        x_test = pd.concat([x_m.iloc[l_m:, :], x_n.iloc[l_n:, :]])\n",
    "        y_train = pd.concat([y_m.iloc[:l_m], y_n.iloc[:l_n]])\n",
    "        y_test = pd.concat([y_m.iloc[l_m:], y_n.iloc[l_n:]])\n",
    "    \n",
    "        x_test.reset_index(drop = True, inplace = True)\n",
    "        y_test.reset_index(drop = True, inplace = True)\n",
    "        x_train.reset_index(drop = True, inplace = True)\n",
    "        y_train.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "        y_test=y_test.astype('int')\n",
    "        y_train=y_train.astype('int')\n",
    "            \n",
    "        classifier = classifier.fit(x_train, y_train)\n",
    "        \n",
    "        y_pred = classifier.predict(x_test)\n",
    "        \n",
    "        new_result = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        p_hit_m = new_result.item(3)/(new_result.item(3) + new_result.item(2))\n",
    "        \n",
    "        p_hit_n = new_result.item(0)/(new_result.item(1) + new_result.item(0))\n",
    "        \n",
    "        if to_print_boxplot:\n",
    "            distribution.loc[len(distribution)] = [p_hit_m, p_hit_n, (p_hit_m+p_hit_n)/2]\n",
    "        \n",
    "        s = s + new_result\n",
    "    \n",
    "    if to_print_boxplot:\n",
    "        string = path + 'Boxplots\\\\' + str(standartization_var) + '\\\\' + str(clas_name) + '\\\\' + str(par) + '_' + str(n) + \".pdf\"\n",
    "        distribution.boxplot()\n",
    "        plt.savefig('string', format='pdf')\n",
    "    s = s/n\n",
    "    \n",
    "    return s\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "data_with_coef = pd.DataFrame(columns = ['Volume', 'Volume variance',\n",
    "            \n",
    "            'Open with volume', 'Open with volume variance', 'High with volume', 'High with volume variance', \n",
    "                                         'Low with volume',\n",
    "            'Low with volume variance', 'Close with volume', \n",
    "                                         'Close with volume variance',\n",
    "            \n",
    "            'Open minus close with volume', 'Open minus close with volume variance', 'High minus close with volume',\n",
    "                                         'High minus close with volume variance', \n",
    "            'Low minus close with volume', 'Low minus close with volume variance',\n",
    "            \n",
    "            'Volume percentage change', 'Volume percentage change variance',\n",
    "            \n",
    "            'Open percentage change', 'Open percentage change variance', 'High percentage change',\n",
    "            'High percentage change variance', 'Low percentage change', 'Low percentage change variance',\n",
    "            'Close percentage change', 'Close percentage change variance', 'Probability'])\n",
    "\n",
    "# def create_KNN_classifier(par):\n",
    "#     return KNeighborsClassifier(n_neighbors = par, weights = 'distance')\n",
    "\n",
    "class NCAClassifier:\n",
    "    def __init__(self, par):\n",
    "        self.knn = KNeighborsClassifier(n_neighbors = par[1], weights = 'distance')\n",
    "        self.nca = NeighborhoodComponentsAnalysis(n_components = par[0], random_state=42, max_iter = 100000, tol = 1e-10000000000, init = 'random')\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        self.nca.fit(x_train, y_train) \n",
    "        x_train = self.nca.transform(x_train)\n",
    "        self.knn.fit(x_train, y_train) \n",
    "        return self\n",
    "    def predict(self, x_test):\n",
    "        x_test = self.nca.transform(x_test)\n",
    "        return self.knn.predict(x_test)\n",
    "\n",
    "def create_NCA_classifier(par):\n",
    "    return NCAClassifier(par)\n",
    "\n",
    "class NBClassifier:\n",
    "    def __init__(self):\n",
    "        self.nb = GaussianNB()\n",
    "        self.pt = PowerTransformer()\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.pt.fit(x_train)\n",
    "        self.nb.fit(self.pt.transform(x_train), y_train)\n",
    "        return self\n",
    "    def predict(self, x_test):\n",
    "        return self.nb.predict(self.pt.transform(x_test))\n",
    "\n",
    "def create_NB_classifier(par):\n",
    "    return NBClassifier()\n",
    "\n",
    "# def create_DT_classifier(par):\n",
    "#     return DecisionTreeClassifier()\n",
    "\n",
    "def create_GB_classifier(par):\n",
    "    return GradientBoostingClassifier(n_estimators=100000, learning_rate=0.1, random_state=0, loss = \"exponential\", tol=1e-10000)\n",
    "\n",
    "class ANNClassifier:\n",
    "    def __init__(self, par):\n",
    "        self.mlp = MLPClassifier(alpha = 0.1, hidden_layer_sizes=par, max_iter=200000000, tol=1e-10000000000000, solver='lbfgs', max_fun=15000000000, )\n",
    "        self.pt = PowerTransformer()\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.pt.fit(x_train)\n",
    "        self.mlp.fit(self.pt.transform(x_train), y_train)\n",
    "        return self\n",
    "    def predict(self, x_test):\n",
    "        return self.mlp.predict(self.pt.transform(x_test))\n",
    "\n",
    "def create_ANN_classifier(par):\n",
    "    return ANNClassifier(par)\n",
    "\n",
    "def create_classifier(clas_name, par):\n",
    "#     if clas_name == \"KNN\":\n",
    "#         return create_KNN_classifier(par)\n",
    "    if clas_name == \"NCA\":\n",
    "        return create_NCA_classifier(par)\n",
    "    if clas_name == \"NB\":\n",
    "        return create_NB_classifier(par)\n",
    "#     if clas_name == \"DT\":\n",
    "#         return create_DT_classifier(par)\n",
    "    if clas_name == \"GB\":\n",
    "        return create_GB_classifier(par)\n",
    "    if clas_name == \"ANN\":\n",
    "        return create_ANN_classifier(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_tester(clas_name, n, standartization_var, remove = [], pars = [0], to_print_boxplot = False):\n",
    "    \n",
    "    if len(data_with_coef) != 0:\n",
    "        data_with_coef.drop(data_with_coef.index, inplace=True)\n",
    "    \n",
    "    cont = True\n",
    "    first = True\n",
    "    \n",
    "    p = 0.01\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for par in pars:\n",
    "        classifier = create_classifier(clas_name, par)\n",
    "        new_result = classifier_effectivness(classifier, clas_name, n, standartization_var, remove, par, to_print_boxplot)\n",
    "        \n",
    "        sens = new_result.item(3)/(new_result.item(3) + new_result.item(2))\n",
    "        \n",
    "        spes = new_result.item(0)/(new_result.item(1) + new_result.item(0))\n",
    "        \n",
    "        res.append([par, [sens, spes]])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-For GB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set = par_tester('GB', 1, 12)\n",
    "print(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-For ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = [(450), (225, 225), (150, 150, 150)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set = par_tester('ANN', 1, 12, [], pars)\n",
    "print(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-For NB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_tester('NB', 10, 12, [2, 3, 4, 5, 6, 8, 9, 11, 13, 17, 23])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-For NCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = []\n",
    "for i in range(1, 26):\n",
    "    for j in range(1, 9):\n",
    "        pars.append([i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = par_tester('NCA', 1, 12, [], pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_array = [x[1][0] for x in res]\n",
    "specificity_array = [x[1][1] for x in res]\n",
    "accuracy_array = [(x[1][0] + x[1][1])/2 for x in res]\n",
    "\n",
    "arrays = [sensitivity_array, specificity_array, accuracy_array]\n",
    "\n",
    "for i in range(3):\n",
    "    array = arrays[i]\n",
    "    new_array = np.reshape(array, (25, 8))\n",
    "    data = pd.DataFrame(new_array)\n",
    "    data.style.background_gradient(cmap='coolwarm')\n",
    "    data.to_excel('NCA Parameters\\\\' + str(i) + \".xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing subsets for NB algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def almost_powerset(my_list):\n",
    "    return list(chain.from_iterable(combinations(my_list, r) for r in range(0, len(my_list) - 3)))\n",
    "\n",
    "def get_the_best_subsets(var, iterations, algorithms, remove = [], number_of_variables):\n",
    "    number_of_variables = 0\n",
    "    if var == 12:\n",
    "        number_of_variables = 26\n",
    "    elif var == 3:\n",
    "        number_of_variables = 7\n",
    "    elif var == 4:\n",
    "        number_of_variables = 7\n",
    "    else:\n",
    "        number_of_variables = 6\n",
    "    \n",
    "    my_set = list(range(number_of_variables))\n",
    "    \n",
    "    my_set = [x for x in my_set if x not in remove]\n",
    "\n",
    "\n",
    "    alm_powerset = almost_powerset(my_set)\n",
    "    \n",
    "    print(alm_powerset)\n",
    "\n",
    "    \n",
    "    for algorithm in algorithms:\n",
    "        results = []\n",
    "        for subset in alm_powerset:\n",
    "            subset = list(subset)\n",
    "            for x in remove:\n",
    "                subset.append(x)\n",
    "            results.append([subset , par_tester(algorithm, iterations, var, subset)])\n",
    "    \n",
    "        results.sort(reverse=True, key=lambda x: x[1][0][1][0] + x[1][0][1][1])\n",
    "        \n",
    "        new_results = results.copy()\n",
    "\n",
    "        for x in results:\n",
    "            if x[1][0][0] <= 0.5 or x[1][0][1] <= 0.5:\n",
    "                new_results.remove(x)\n",
    "    return new_results[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 15, 17, 18, 19, 23, 25]\n",
    "results_nb = get_the_best_subsets(12, 1, ['NB'], [2, 3, 4, 5, 7, 8, 9, 13, 15, 17, 19, 23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_best = []\n",
    "\n",
    "for [subset, probs] in results_nb[0:100]:\n",
    "    new_prob = par_tester('NB', 100, 12, subset)\n",
    "    the_best.append([subset, new_b])\n",
    "    \n",
    "the_best.sort(reverse=True, key=lambda x: x[1][1][0] + x[1][1][1])\n",
    "\n",
    "for i in the_best:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBClassifier:\n",
    "    def __init__(self):\n",
    "        self.nb = GaussianNB()\n",
    "        self.pt = PowerTransformer()\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.pt.fit(x_train)\n",
    "        self.nb.fit(self.pt.transform(x_train), y_train)\n",
    "        return self\n",
    "    def predict(self, x_test):\n",
    "        return self.nb.predict(self.pt.transform(x_test))\n",
    "\n",
    "def create_GB_classifier(par):\n",
    "    return GradientBoostingClassifier(n_estimators=100000, learning_rate=0.01, random_state=0, loss = \"exponential\", tol=1e-10000)\n",
    "    \n",
    "standartization_var = 12\n",
    "\n",
    "manip_days = pd.read_excel(\"New All Normalized Data\\\\Var\" + str(standartization_var) + \"\\\\\" + \"Manipulated Days\" + \".xlsx\")\n",
    "not_manip_days = pd.read_excel(\"New All Normalized Data\\\\Var\" + str(standartization_var) + \"\\\\\" + \"Not Manipulated Days\" + \".xlsx\")\n",
    "\n",
    "# grov = pd.read_excel(\"INTV.xlsx\")\n",
    "# grov = pd.read_excel(\"APG.AX.xlsx\")\n",
    "grov = pd.read_excel(\"0567.HK.xlsx\")\n",
    "\n",
    "# print(grov)\n",
    "\n",
    "grov = data_standartization_var12(grov, 20)\n",
    "\n",
    "print(grov)\n",
    "\n",
    "\n",
    "\n",
    "train_frac = 1   \n",
    "\n",
    "indexes_to_use = list(range(26))\n",
    "indexes_to_use = [0, 10, 11, 14, 21]\n",
    "\n",
    "all_res = pd.DataFrame(columns=['Accuracy', 'Sensitivity', 'Specificity'])\n",
    "\n",
    "class NCAClassifier:\n",
    "    def __init__(self, par):\n",
    "        self.knn = KNeighborsClassifier(n_neighbors = par[1], weights = 'distance')\n",
    "        self.nca = NeighborhoodComponentsAnalysis(n_components = par[0], random_state=42, max_iter = 100000, tol = 0, init = 'random')\n",
    "        self.par = par\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        self.nca.fit(x_train, y_train)\n",
    "        \n",
    "        print(self.nca.n_iter_)\n",
    "        \n",
    "        x_train = self.nca.transform(x_train)\n",
    "        self.knn.fit(x_train, y_train) \n",
    "        return self\n",
    "    def predict(self, x_test):\n",
    "        x_test = self.nca.transform(x_test)\n",
    "        return self.knn.predict(x_test)\n",
    "\n",
    "class ANNClassifier:\n",
    "    def __init__(self):\n",
    "        tuple = (225, 225)\n",
    "        self.mlp = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes=tuple, max_iter=20000000000, tol=0, solver='lbfgs', max_fun=150000000000000000000, )\n",
    "        self.pt = PowerTransformer()\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.pt.fit(x_train)\n",
    "        self.mlp.fit(self.pt.transform(x_train), y_train)\n",
    "        print(self.mlp.n_iter_)\n",
    "        return self\n",
    "    def predict(self, x_test):\n",
    "        return self.mlp.predict(self.pt.transform(x_test))\n",
    "\n",
    "classifier = ANNClassifier()\n",
    "# classifier = NCAClassifier([12, 6])\n",
    "    \n",
    "n = 1\n",
    "\n",
    "for i in range(0, n):\n",
    "        print(i)\n",
    "        manip_days = manip_days.sample(frac=1).reset_index(drop = True)\n",
    "        not_manip_days = not_manip_days.sample(frac=1).reset_index(drop = True)\n",
    "    \n",
    "        x_m = manip_days.iloc[:, indexes_to_use]\n",
    "        x_n = not_manip_days.iloc[:, indexes_to_use].iloc[0:len(x_m)]\n",
    "        \n",
    "        print(len(x_m))\n",
    "        print(len(x_n))\n",
    "        \n",
    "        y_m = manip_days.iloc[:, 26]\n",
    "        y_n = not_manip_days.iloc[:, 26].iloc[0:len(x_m)]\n",
    "\n",
    "#         l_m = int(len(x_m)*train_frac)\n",
    "#         l_n = int(len(x_n)*train_frac)\n",
    "\n",
    "        x_train = pd.concat([x_m, x_n])\n",
    "        y_train = pd.concat([y_m, y_n])\n",
    "    \n",
    "        x_test = grov.iloc[:, indexes_to_use]\n",
    "        y_test = grov.iloc[:, 26]\n",
    "    \n",
    "        x_test.reset_index(drop = True, inplace = True)\n",
    "        y_test.reset_index(drop = True, inplace = True)\n",
    "        x_train.reset_index(drop = True, inplace = True)\n",
    "        y_train.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "        y_test=y_test.astype('int')\n",
    "        y_train=y_train.astype('int')\n",
    "            \n",
    "        classifier = classifier.fit(x_train, y_train)\n",
    "        \n",
    "        y_pred = classifier.predict(x_test)\n",
    "        \n",
    "        together = pd.concat([pd.DataFrame(y_pred), pd.DataFrame(y_test)], axis=1)\n",
    "        print(together)\n",
    "        \n",
    "        for ind in together.index:\n",
    "            if together.loc[ind, 0] != together.iloc[ind, 1]:\n",
    "                print(ind)\n",
    "        \n",
    "        new_result = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        p_hit_m = new_result.item(3)/(new_result.item(3) + new_result.item(2))\n",
    "        \n",
    "        p_hit_n = new_result.item(0)/(new_result.item(1) + new_result.item(0))\n",
    "        \n",
    "        all_res.loc[len(all_res)] = [(new_result.item(3) + new_result.item(0))/len(together), p_hit_m, p_hit_n]\n",
    "print(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "history": [],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
